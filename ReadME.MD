<div align="center">

<h1>ğŸ§  SADOP</h1>
<h3>Smart AI-Driven Database Optimization Platform</h3>

<p>
An academic project combining <b>Databases</b>, <b>Machine Learning</b>, <b>Reinforcement Learning</b>, and <b>LLMs</b> to analyze, predict, and optimize MySQL performance.
</p>

<hr/>

<!-- Tech Stack Icons -->
<p>
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/mysql/mysql-original.svg" width="50"/>
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg" width="50"/>
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/pytorch/pytorch-original.svg" width="50"/>
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/tensorflow/tensorflow-original.svg" width="50"/>
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/git/git-original.svg" width="50"/>
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linux/linux-original.svg" width="50"/>
</p>

</div>

---

## ğŸ§  Project Overview

**SADOP (Smart AI-Driven Database Optimization Platform)** is a system designed to:

- Monitor MySQL performance
- Detect slow and inefficient queries
- Predict performance problems using Machine Learning
- Automatically recommend database indexes using Reinforcement Learning
- Interact with the database using Natural Language (LLMs)

---

## ğŸ§± Phase 1 â€” Environment & Tools (M1)

### ğŸ¯ Goal

Prepare a **professional development environment** capable of:

- Running MySQL
- Generating large datasets
- Training ML & RL models
- Interacting with LLMs

### âœ… Completed Tasks

#### ğŸ’» Operating System

- Windows 11 Pro (development environment)

#### ğŸ›¢ï¸ Database

- MySQL Server 8.x
- Running on port **3307**
- Managed via MySQL Workbench

#### ğŸ” Security

- Root account secured
- Dedicated database user created: `sadop_user`
- Restricted privileges applied

#### ğŸ Python Environment

- Python 3.12
- Conda virtual environment: **SADOP**
- JupyterLab / Jupyter Notebook configured

#### ğŸ“¦ Installed Libraries

**Core**

- mysql-connector-python
- pandas, numpy

**Machine Learning**

- scikit-learn
- tensorflow / pytorch

**Reinforcement Learning**

- gymnasium
- stable-baselines3

**LLM**

- langchain
- Ollama (local LLM)

#### ğŸ“ Project Structure

```
SADOP/
â”œâ”€â”€ db/
â”‚ â””â”€â”€ sadop_db_backup.sql
â”œâ”€â”€ monitoring/
â”‚ â”œâ”€â”€ performance_schema_queries.sql
â”‚ â””â”€â”€ slow_log_parser.py
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ accounts.csv
â”‚ â”œâ”€â”€ generated_queries.csv
â”‚ â”œâ”€â”€ logs.csv
â”‚ â”œâ”€â”€ ml_features_clean.csv
â”‚ â”œâ”€â”€ ml_features.csv
â”‚ â”œâ”€â”€ slow_query_metrics-Copy1.csv
â”‚ â”œâ”€â”€ slow_query_metrics.csv
â”‚ â”œâ”€â”€ transactions.csv
â”‚ â”œâ”€â”€ users.csv
â”‚ â”œâ”€â”€ X_test.csv
â”‚ â”œâ”€â”€ X_train.csv
â”‚ â”œâ”€â”€ y_test_cls.csv
â”‚ â”œâ”€â”€ y_test_reg.csv
â”‚ â”œâ”€â”€ y_train_cls.csv
â”‚ â””â”€â”€ y_train_reg.csv
â”œâ”€â”€ ML/
â”‚ â”œâ”€â”€ 1_Data Pipeline for AI .ipynb
â”‚ â”œâ”€â”€ 2_exploratory_analysis.ipynb
â”‚ â”œâ”€â”€ 3_Normalize_Numeric_Features _Train_Test Split.ipynb
â”‚ â”œâ”€â”€ 4_Verify Data Quality & Consistency.ipynb
â”‚ â”œâ”€â”€ 5_ML Diagnostic Engine â€” Predicting Query Performance.ipynb
â”‚ â”œâ”€â”€ models/
â”‚ â”‚   â”œâ”€â”€ logistic_regression.pkl
â”‚ â”‚   â””â”€â”€ random_forest.pkl
â”‚ â””â”€â”€ text.txt
â”œâ”€â”€ RL/
â”‚ â”œâ”€â”€ 00_rl_index_optimization_agent.ipynb
â”‚ â”œâ”€â”€ 01_EXPLAIN_Extraction.ipynb
â”‚ â”œâ”€â”€ 02_Index Recommendation Engine.ipynb
â”‚ â””â”€â”€ rl_tensorboard/
â”‚     â”œâ”€â”€ PPO_1/
â”‚     â”‚   â””â”€â”€ events.out.tfevents.1766342679.DESKTOP-GK9TB81.8128.0
â”‚     â”œâ”€â”€ PPO_2/
â”‚     â”‚   â””â”€â”€ events.out.tfevents.1766343461.DESKTOP-GK9TB81.8128.1
â”‚     â”œâ”€â”€ PPO_3/
â”‚     â”‚   â””â”€â”€ events.out.tfevents.1766343473.DESKTOP-GK9TB81.8128.2
â”‚     â”œâ”€â”€ PPO_4/
â”‚     â”‚   â””â”€â”€ events.out.tfevents.1766343521.DESKTOP-GK9TB81.45172.0
â”‚     â”œâ”€â”€ PPO_5/
â”‚     â”‚   â””â”€â”€ events.out.tfevents.1766343583.DESKTOP-GK9TB81.45172.1
â”‚     â”œâ”€â”€ PPO_6/
â”‚     â”‚   â””â”€â”€ events.out.tfevents.1766343645.DESKTOP-GK9TB81.45172.2
â”‚     â”œâ”€â”€ PPO_7/
â”‚     â”‚   â””â”€â”€ events.out.tfevents.1766343848.DESKTOP-GK9TB81.45172.3
â”‚     â””â”€â”€ PPO_8/
â”‚         â””â”€â”€ events.out.tfevents.1766352475.DESKTOP-GK9TB81.28192.0
â”œâ”€â”€ llm/
â”‚ â”œâ”€â”€ prompt_templates.py
â”‚ â””â”€â”€ sql_explainer.py
â”œâ”€â”€ api/
â”‚ â””â”€â”€ app.py
â”œâ”€â”€ report/
â”‚ â””â”€â”€ (empty)
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_environment_check.ipynb
â”‚ â”œâ”€â”€ 02_mysql_connection.ipynb
â”‚ â”œâ”€â”€ 03_generate_fake_data.ipynb
â”‚ â”œâ”€â”€ 04_load_csv_into_mysql.ipynb
â”‚ â”œâ”€â”€ 05_MySQL_Monitoring_Setup.ipynb
â”‚ â”œâ”€â”€ 06_Enable_Performance_Schema.ipynb
â”‚ â”œâ”€â”€ 07_Capture Performance Metrics.ipynb
â”‚ â””â”€â”€ 08_Generate Realistic Slow Queries and Metrics.ipynb
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Projet-Homework.pdf
â”œâ”€â”€ PROJECT_STRUCTURE.md
â””â”€â”€ ReadME.MD

```

---

## ğŸ§± Phase 2 â€” Database Design & Data Generation (M2)

### ğŸ¯ Goal

Create a **realistic, large, and normalized database** able to:

- Stress MySQL
- Generate performance issues
- Feed ML & RL models

### ğŸ¦ Use Case

**Banking System**

### ğŸ§© Database Design

#### Main Entities

- Users
- Accounts
- Transactions
- Logs

#### Design Quality

- ER diagram created
- Schema normalized to **3NF**
- Primary keys defined
- Foreign keys enforced
- Basic indexes added

### ğŸ§  Schema Implementation

- SQL DDL scripts written
- Tables created via Jupyter
- User-based access (no root usage)

### ğŸ§ª Data Generation

#### Small Dataset

- Inserted for testing
- Data consistency validated
- Errors fixed (phone number length)

#### Large Dataset (Production-Scale)

- Users: ~10,000
- Accounts: ~20,000+
- Transactions: **100,000+**
- Logs: **50,000 â€“ 100,000**
- Inserted **in batches**
- With realistic timestamps and values

#### CSV Export

- Tables exported for offline use:
  - `data/users.csv`
  - `data/accounts.csv`
  - `data/transactions.csv`
  - `data/logs.csv`

### ğŸ’¾ Backup

- Full MySQL dump created: `db/sadop_db_backup.sql`

âœ… **Milestone M2 completed**

---

## ğŸ§± Phase 3 â€” Monitoring & Performance Data (M3)

### ğŸ¯ Goal

Collect **real MySQL performance signals** for AI models.

### ğŸ” Completed Steps

- Enabled MySQL `slow_query_log`
- Aggressive threshold set (queries slower than **0.4s**)
- Located slow query log file
- Simulated realistic queries on CSV + captured:
  - Execution time
  - Rows examined
  - Number of joins
- Simulated missing indexes and traffic spikes
- Captured CPU & memory usage per query
- Saved metrics to CSV:
  - `data/slow_query_metrics.csv`
  - `data/slow_query_metrics_realistic.csv`
  - `data/slow_query_metrics_final.csv`
  - `data/ml_features.csv`

This enables:

- Real workload observation
- Performance labeling
- ML/RL feature extraction

### ğŸ”§ Jupyter Notebooks

- 01_environment_check.ipynb
- 02_mysql_connection.ipynb
- 03_generate_fake_data.ipynb
- 04_generate_data_Frame.ipynb
- 06_MySQL_Monitoring_Setup.ipynb
- 07_Enable_Performance_Schema.ipynb
- 08_Capture_Performance_Metrics.ipynb
- 09_Simulate_Queries_and_Capture_Metrics.ipynb
- 10_Generate_Realistic_Slow_Queries_and_Metrics.ipynb

---

## ğŸ§± Phase 4 â€” Feature Engineering & ML Pipeline (Stage 10 â†’ 13)

### ğŸ¯ Goal

Prepare a **fully enhanced, ML-ready dataset** including:

- Structural query features
- Realistic performance metrics
- System load (CPU/memory)
- Adaptive slow query labeling
- Normalized numeric features
- Train/test split

### ğŸ”¹ Stage 10: Generate Realistic Slow Queries & Metrics

- Heavy queries with Users â†’ Accounts â†’ Transactions joins
- Aggregated transactions with `SUM` per account
- Simulated missing index by shuffling transactions
- Traffic spikes: repeated queries 3x with slight time variability
- Captured CPU & memory usage per query using Python `psutil`
- Shuffled all queries
- Saved as `data/slow_query_metrics_final.csv`

### ğŸ”¹ Stage 11: Feature Engineering & Structural Encoding

- Added structural features:
  - `has_sum`, `has_group_by`, `has_where`
  - `tables_count` (joins + 1)
  - `query_length`
- Saved dataset as `data/ml_features.csv`

### ğŸ”¹ Stage 12: Exploratory Analysis & Linear Regression

- Plotted histogram of `query_time` â†’ identified fast vs slow clusters
- Boxplot of `rows_examined` vs `is_slow` â†’ confirmed correlation
- Linear regression on numeric + structural features â†’ identified key predictors
- Validated that features can predict query performance

### ğŸ”¹ Stage 13: Normalize & Train/Test Split

- Log-transformed `query_time` to smooth gaps
- Selected numeric + categorical features
- Standardized numeric features using StandardScaler
- Split dataset 80/20 train/test
- Saved preprocessed datasets:
  - `X_train.csv`, `X_test.csv`
  - `y_train_reg.csv`, `y_test_reg.csv`
  - `y_train_cls.csv`, `y_test_cls.csv`

**Result:** Fully ML-ready dataset including **all performance, structural, and system metrics**.

---

## ğŸ“‚ Files Added / Generated So Far

- `data/users.csv`
- `data/accounts.csv`
- `data/transactions.csv`
- `data/logs.csv`
- `data/query_metrics.csv`
- `data/slow_query_metrics.csv`
- `data/slow_query_metrics_realistic.csv`
- `data/slow_query_metrics_final.csv`
- `data/ml_features.csv`
- `db/sadop_db_backup.sql`

---

## ğŸ§­ Next Steps

- Task 41+ (M4): Define features for ML pipeline
- Train diagnostic ML models
- Build RL index optimization agent
- Integrate LLM natural language interface
- Evaluate and document end-to-end system

---

<div align="center">
<b>SADOP â€” Turning database performance into intelligent decisions.</b>
</div>
